name: benchmark-prompt-quality

on:
  workflow_dispatch:

jobs:
  prompt-benchmark:
    runs-on: macos-14
    timeout-minutes: 45
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Build release binaries
        run: swift build -c release

      - name: Run prompt benchmark suite
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python3 scripts/bench_prompt_suite.py \
            --drafts-file bench/fixtures/profiles/profile_set.txt \
            --preamble-variants "core=bench/preambles/core.md,large_opt=bench/preambles/large-optimized-v1.md,extended=bench/preambles/extended.md" \
            --web-search-modes "disabled,cached" \
            --models gpt-5.3-codex-spark \
            --efforts low \
            --backend both \
            --pairwise \
            --out-dir tmp/bench-prompt-ci

      - name: Upload prompt benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: turbodraft-prompt-benchmark-results
          path: |
            tmp/bench-prompt-ci
