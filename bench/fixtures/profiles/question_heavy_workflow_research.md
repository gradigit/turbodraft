Let's run some research in parallel using the study skill too. So let's run research on this actual prompt reviewing checklist and see if this checklist that we're using to review the baseline prompt is trustworthy and objective.

And at the same time, let's also run research on the baseline prompt and whether this kind of format is the actual optimal engineered prompt format. Because we're not talking about like planning or creating a task list or PRDs or epics or user stories or anything like that. We're really focused on prompt, prompting itself as an art. 

I think the user inputs to request being so many requests is actually not too bad. Because ultimately again, I think this would depend on the research, but ultimately, that amount of questions might be what is actually needed to produce good output. At least according to the agent. And in the initial plan I mentioned that I eventually want to expand on this uh prompt engineering prompt pad editor concept, where there will be a chat window or chat panel that I can use to chat with the AI that's engineering the prompt. So as an example, let's say the agent, the prompt engineering agent gave me this baseline prompt. Well, along with this, I think the prompt engineering agent can also ask me some questions, like in plan mode or whatever, and I can just go down the checklist and answer those questions, and then the prompt engineering agent would then refine the prompt, and after as many iterations as I want, the prompt will be perfected, and then I just save and quit the editor, and this perfected prompt is now injected into the CLI. I think that's a good workflow and this is kind of what I already do, but having this dedicated editor will make it a lot better I think. We can run research on whether this is a good workflow as well.
